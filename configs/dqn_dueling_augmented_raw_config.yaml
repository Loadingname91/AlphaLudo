# Dueling Double DQN Agent Configuration (Augmented Raw State + Scaled Dense Rewards)
# Implements: Dueling DQN + Double Learning + Prioritized Replay (PER) + N-Step Returns
# State Representation: Augmented Raw (90-dim)
# Reward Schema: Scaled Dense (Win=300, Goal=100, Kill=50, Progress=0.01)

# Experiment settings
experiment:
  name: "dqn_dueling_augmented_raw_scaled"
  seed: 42
  output_dir: "results"

# Agent configuration
agent:
  type: "dqn"
  seed: 42
  player_id: 0
  debug_scores: true
  state_representation: "augmented_raw"  # <--- NEW PARAMETER
  
  # Training Hyperparameters
  learning_rate: 0.0001
  gamma: 0.99
  train_freq: 4                # Train every 4 steps to reduce CPU load
  
  # Exploration (Epsilon-Greedy)
  epsilon_start: 1.0
  epsilon_end: 0.05
  # Slightly slower decay to match larger input complexity (reaches ~0.05 around 60k eps)
  epsilon_decay: 0.99995
  
  # Experience Replay
  batch_size: 128
  buffer_size: 80000
  
  # Target Network
  target_update_freq: 1000
  
  # Device
  device: "cpu"
  
  # PER parameters
  per_alpha: 0.6
  per_beta_start: 0.4
  per_beta_end: 1.0
  per_epsilon: 0.000001

# Environment configuration
environment:
  reward_schema: "dense"  # Enhanced dense rewards with progress-based rewards for better convergence
  player_id: 0
  render: false

# Training configuration
training:
  num_episodes: 75000
  max_steps_per_episode: 10000
  log_interval: 2000
  save_interval: 5000
  checkpoint_dir: "checkpoints/dqn_dueling_augmented_raw_scaled"  # <--- UPDATED PATH
  enable_score_debug: true
  
  # Learning schedule
  warmup_episodes: 2000
  min_buffer_size: 1000


